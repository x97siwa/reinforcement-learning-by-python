{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day1 強化学習の位置付けを知る"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "         *                 *                  *              *\n",
    "                                                      *             *\n",
    "                        *            *                             ___\n",
    "  *               *                                          |     | |\n",
    "        *              _________##                 *        / \\    | |\n",
    "                      @\\\\\\\\\\\\\\\\\\##    *     |              |--o|===|-|\n",
    "  *                  @@@\\\\\\\\\\\\\\\\##\\       \\|/|/            |---|   |d|\n",
    "                    @@ @@\\\\\\\\\\\\\\\\\\\\\\    \\|\\\\|//|/     *   /     \\  |w|\n",
    "             *     @@@@@@@\\\\\\\\\\\\\\\\\\\\\\    \\|\\|/|/         |  R    | |b|\n",
    "                  @@@@@@@@@----------|    \\\\|//          |  L    |=| |\n",
    "       __         @@ @@@ @@__________|     \\|/           |       | | |\n",
    "  ____|_@|_       @@@@@@@@@__________|     \\|/           |_______| |_|\n",
    "=|__ _____ |=     @@@@ .@@@__________|      |             |@| |@|  | |\n",
    "____0_____0__\\|/__@@@@__@@@__________|_\\|/__|___\\|/__\\|/___________|_|_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 準備\n",
    "\n",
    "以下の Python ライブラリを使用します．\n",
    "\n",
    "- `gym`\n",
    "- `numpy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 環境とは？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "強化学習では，「環境」が与えられます．この環境に対する，より良い行動パターンを見つけ出すことが強化学習の目的です．\n",
    "\n",
    "> 環境とは「行動」と行動に応じた「状態」の変化が定義されており，ある状態への到達に対し「報酬」が与えられる空間のことです．端的には，ゲームのようなものです．ゲームでは，ボタンを押したらキャラクターがジャンプしたりします．「ボタンを押す」のが行動であり，「キャラクターがジャンプする」のが状態の変化に相当します．そしてゴールに到達できれば「報酬」が得られます． *( p.15 )*\n",
    "\n",
    "ここでは，**FrozenLake-v0**（以下，FL）というテキストゲームを例にとって「環境」とは何かを説明します．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FrozenLake-v0 について\n",
    "\n",
    "FL の説明は[ココ](https://gym.openai.com/envs/FrozenLake-v0/)に乗っています．リンク先には実際のプレイ動画もあります．「凍った湖の上に落としてしまったフリスビーを取りに行く」という筋書きらしいです．湖は次のような４ｘ４のマップで表されます．\n",
    "\n",
    "```\n",
    "SFFF       (S: starting point, safe)\n",
    "FHFH       (F: frozen surface, safe)\n",
    "FFFH       (H: hole, fall to your doom)\n",
    "HFFG       (G: goal, where the frisbee is located)\n",
    "```\n",
    "\n",
    "スタート地点である `S` のマスから出発して，ゴール地点（フリスビーのある場所）である `G`のマスに辿り着けばゲームグリアです．ただし，所々に溶けて穴が空いているマス `H` があり，ここに落ちてしまうとゲームオーバーになります．移動方法は「上（下・左・右）に進む」の４つです．しかし，`F` のマスは凍っていてよく滑るので，望んだ方向に進める確率は１/３です．例えば，上に進もうとした場合逆方向である下以外（左・右）にも同じ確率で進む可能性があります．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FrozenLake-v0 で遊ぶ\n",
    "\n",
    "OpenAI が **gym** という Python ライブラリを公開しています．gym には強化学習に使える様々な環境が用意されていて，その中に FL もあります．これをこのノート用に扱いやすくしたラッパークラスを `day1/frozen_lake_wrap.py` に用意しました．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from frozen_lake_wrap import FrozenLake, Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`FrozenLake` がゲーム本体，`Action` は列挙型で移動方法が定義されています．実際に動かしてみましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = FrozenLake(is_slippery=False)\n",
    "env.reset()\n",
    "env.env.render()\n",
    "env.step(Action.DOWN)\n",
    "env.env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コンストラクタで `is_slippery=False` を指定すると，`F` のマスで滑らなくなる，つまり望んだ方向に必ず進めるようになります．FrozenLake のメソッドをいくつか紹介します．\n",
    "\n",
    "- `env.reset()` ゲームを初期化する\n",
    "- `env.step()` １マス移動する\n",
    "- `env.render()` 盤面を描画する\n",
    "- `env.close()` ゲームを終了する\n",
    "\n",
    "`step()` には `Action.UP`,`Action.DOWN`,`Action.RIGHT`,`Action.LEFT` の内１つを指定します．上記のコードでは `Action.DOWN` を指定したので，スタート地点 $(0,0)$ から $(1,0)$ に移動したわけです．ここで，位置座標は（行，列）としていますから，$(1,0)$ は１行目の０列目のことです．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 環境 ＝ マルコフ決定過程\n",
    "\n",
    "強化学習における環境とは，**マルコフ決定過程（Markov Decision Process: MDP）**のことです．マルコフ決定過程は，４つの要素の組 $ \\langle S, A, T, R \\rangle $ で表されます．$S$ は状態集合，$A$ は行動集合，$T$ は遷移関数，$R$ は報酬関数です．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 状態集合\n",
    "\n",
    "FL において，「状態」とは位置座標のことです．このゲームは４×４マスなので，状態は全部で 16 通りあるわけです．状態集合 $S$ とはこれら全ての状態を要素とする集合を指します．\n",
    "\n",
    "$$ S = \\left\\{ (i,j) \\mid 0 \\leq i,j < 4 \\right\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 行動集合\n",
    "\n",
    "FL の「行動」は「上（下・左・右）への移動」です．これをそれぞれ $a_U$，$a_D$，$a_L$，$a_R$ とすると，行動集合 $A$ は次のようになります．\n",
    "\n",
    "$$ A = \\left\\{ a_U, a_D, a_L, a_R \\right\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 遷移関数\n",
    "\n",
    "遷移関数は，状態 $s$ で行動 $a$ を取ったとき状態 $s'$ に遷移する確率を表します．\n",
    "\n",
    "$$T : S \\times A \\times S \\rightarrow [0,1];\\hspace{4pt} (s,a,s') \\mapsto T(s' \\mid s,a)$$\n",
    "\n",
    "FL は望んだ方向に進める確率が１/３でした．例えば $(2,2)$ から上に移動しようとした場合，$(1,2)$ に移動できる確率は１/３ですから，このとき遷移関数の値は\n",
    "\n",
    "$$ T(s'=(1,2) \\mid s=(2,2), a=a_U) = \\frac{1}{3} $$\n",
    "\n",
    "となります．一方，逆方向 $(3,2)$ に移動することはありませんから，このときの遷移関数の値は $0$ です．\n",
    "\n",
    "$$ T(s'=(3,2) \\mid s=(2,2), a=a_U) = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 報酬関数\n",
    "\n",
    "マルコフ決定過程では，「状態 $s$ で行動 $a$ を取り状態 $s'$ に遷移する」たびに「報酬」が貰えます．この報酬を与えるのが報酬関数です．\n",
    "\n",
    "$$ R : S \\times A \\times S \\rightarrow \\mathbb{R};\\hspace{4pt} (s,a,s') \\mapsto R(s,a,s') $$\n",
    "\n",
    "FL ではゴールのマス $(3,3)$ に移動すると報酬として $1$ が与えられます．\n",
    "\n",
    "$$ R(s,a,s'=(3,3)) = 1$$\n",
    "\n",
    "一方，それ以外のマスに移動したときの報酬は $0$ です．\n",
    "\n",
    "$$ R(s,a,s'\\neq(3,3)) = 0 $$\n",
    "\n",
    "このように，FL の報酬関数 $R$ は遷移先 $s'$ にのみ依存します．強化学習はこの報酬を目安にして学習を行うわけです．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 環境としての FrozenLake-v0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`FrozenLake` の `step()` には３つの返り値があります．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos:(0, 1), reward:0.0, done:False\n"
     ]
    }
   ],
   "source": [
    "env = FrozenLake(is_slippery=True)\n",
    "env.reset()\n",
    "pos, reward, done = env.step(Action.DOWN)\n",
    "print('pos:%s, reward:%s, done:%s'%(pos, reward, done))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pos` は遷移先，`reward` は報酬，`done` はゴールしたかどうかを示すフラグです．遷移先は FL の遷移関数に従って確率的に決定され，報酬は報酬関数によって与えられます．少し遷移関数を見てみましょう．`FrozenLake` では，遷移関数は (４×４)×４×(４×４) の numpy 配列で表されています（遷移関数 $T$ の定義域は $S \\times A \\times S$ です）．例えば $(2,2)$ から上に移動しようとしたときの遷移確率行列は次のようになります．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.        ]\n",
      " [0.         0.         0.33333333 0.        ]\n",
      " [0.         0.33333333 0.         0.33333333]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(env.T[(2,2)][Action.UP])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上 $(1,2)$，左 $(2,1)$，右 $(2,3)$ に移動する確率がそれぞれ $\\frac{1}{3}$，それ以外に移動する確率が $0$ となっていることが分かります．$(0,0)$ から左に移動しようとした場合は少し特殊です．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.66666667 0.         0.         0.        ]\n",
      " [0.33333333 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(env.T[(0,0)][Action.LEFT])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(0,0)$ に遷移する確率が $\\frac{2}{3}$ となっています．FL では，遷移先がマップ外になる場合，元の位置が遷移先となります．$(0,0)$ の左側 $(0,-1)$ はマップの外であり，また上側 $(-1,0)$ もまたマップ外ですから，左に移動する場合と上に移動する場合，遷移先は共に $(0,0)$ になります．なので，合わせて$\\frac{2}{3}$ の確率で $(0,0)$ に遷移する，という計算になるわけです．\n",
    "\n",
    "次は報酬関数を見てみましょう．報酬関数は４×４の numpy 配列です．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(env.R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ゴール地点 $(3,3)$ での報酬が $1$，それ以外の場所では $0$ となっていることが分かります．それでは最後に，`FrozenLake` が強化学習のための環境（マルコフ決定過程）であることを実際に動かしながら確かめてみましょう．以下のセルを実行すると，ランダムな行動を取りながらゲームが進み，その度に遷移先に応じた報酬が貰える様子が見れます．また，その遷移が遷移関数に従っていることも分かるはずです（`is_slippery=True` としているので，望んだ方向に進む確率は $1$ になります）．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Game Over... -----------------\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "------------------------------------------------\n",
      "action:Action.RIGHT at (1, 0),\n",
      "new_state:(1, 1), reward:0.0, done:True,\n",
      "T(s'|s=(1, 0), a=Action.RIGHT):\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def render(env, state, action, new_state, reward, done):\n",
    "    if not done:\n",
    "        print('---------------- Now Playing... ----------------')\n",
    "    elif env.map[new_state[0]][new_state[1]]=='G':\n",
    "        print('----------------- Game Clear! ------------------')\n",
    "    else:\n",
    "        print('----------------- Game Over... -----------------')\n",
    "    \n",
    "    env.render()\n",
    "    print('------------------------------------------------')\n",
    "    print('action:%s at %s,'%(action, state))\n",
    "    print('new_state:%s, reward:%s, done:%s,'%(new_state, reward, done))\n",
    "    if state is not None and action is not None:\n",
    "        print('T(s\\'|s=%s, a=%s):\\n%s'%(state, action, env.T[state][action]))\n",
    "\n",
    "env.close()\n",
    "env = FrozenLake(is_slippery=False)\n",
    "state = env.reset()\n",
    "done = False\n",
    "delay = 5\n",
    "\n",
    "render(env, state, None, None, 0, False)\n",
    "while not done:\n",
    "    sleep(delay)\n",
    "    action = Action.choice()\n",
    "    new_state, reward, done = env.step(action)\n",
    "    clear_output(wait=True)\n",
    "    render(env, state, action, new_state, reward, done)\n",
    "    state = new_state\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
