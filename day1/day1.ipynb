{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day1 強化学習の位置付けを知る"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "         *                 *                  *              *\n",
    "                                                      *             *\n",
    "                        *            *                             ___\n",
    "  *               *                                          |     | |\n",
    "        *              _________##                 *        / \\    | |\n",
    "                      @\\\\\\\\\\\\\\\\\\##    *     |              |--o|===|-|\n",
    "  *                  @@@\\\\\\\\\\\\\\\\##\\       \\|/|/            |---|   |d|\n",
    "                    @@ @@\\\\\\\\\\\\\\\\\\\\\\    \\|\\\\|//|/     *   /     \\  |w|\n",
    "             *     @@@@@@@\\\\\\\\\\\\\\\\\\\\\\    \\|\\|/|/         |  R    | |b|\n",
    "                  @@@@@@@@@----------|    \\\\|//          |  L    |=| |\n",
    "       __         @@ @@@ @@__________|     \\|/           |       | | |\n",
    "  ____|_@|_       @@@@@@@@@__________|     \\|/           |_______| |_|\n",
    "=|__ _____ |=     @@@@ .@@@__________|      |             |@| |@|  | |\n",
    "____0_____0__\\|/__@@@@__@@@__________|_\\|/__|___\\|/__\\|/___________|_|_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 環境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "強化学習では，「環境」が与えられます．この環境の中を動き周りながらより良い行動パターンを学習していくのが強化学習です．\n",
    "\n",
    "> 環境とは「行動」と行動に応じた「状態」の変化が定義されており，ある状態への到達に対し「報酬」が与えられる空間のことです．端的には，ゲームのようなものです．ゲームでは，ボタンを押したらキャラクターがジャンプしたりします．「ボタンを押す」のが行動であり，「キャラクターがジャンプする」のが状態の変化に相当します．そしてゴールに到達できれば「報酬」が得られます． *( p.15 )*\n",
    "\n",
    "例えば $n \\times m$ マスの２次元迷路の場合，状態は位置座標 $(i,j)$ に当たります．なので，状態集合 $S$ は次のように定義できます．\n",
    "\n",
    "$$ S = \\left\\{ (i,j) \\mid 0 \\leq i < n, 0 \\leq j < m \\right\\} $$\n",
    "\n",
    "また，行動 $a$ は上下左右への遷移なので，行動集合 $A$ はこうなります．\n",
    "\n",
    "$$ A = \\left\\{ a_U, a_D, a_L, a_R \\right\\} $$\n",
    "\n",
    "ここで，$a_U$ は「上に移動」を表す行動です(UP の U．以下同様)．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ある状態 $s \\in S$ から次の状態 $s' \\in S$ に遷移するには行動 $a \\in A$ を１つ選択する必要があります．このとき，その選び方にはなにか法則があると仮定しましょう．つまり，ある戦略に基づいて行動を決定するということです．この戦略を関数 $\\pi$ として定義します．\n",
    "\n",
    "$$ a = \\pi(s) $$\n",
    "\n",
    "$\\pi$ は状態を入力にとり，行動を出力します．これで，次にとる行動 $a$ が計算できます．行動 $a$ を実行すると状態が遷移しますが，遷移先の状態は確率的に決まります．つまり，同じ状態で同じ行動をとったとしても，遷移先がいつも同じとは限らないのです．数学的に言うと，遷移先の状態 $s'$ が確立分布\n",
    "\n",
    "$$　Pr(s_{t+1}=s' \\mid s_t=s, a_t=a) = T(a,s)　$$\n",
    "\n",
    "に従うということです．$Pr$ は，直前の状態($s_t$)が $s$，そこでとった行動($a_t$)が $a$ のとき，次の状態($s_{t+1}$)が $s'$ になる確率を表します．ここで，遷移先 $s'$ の確率分布は直前の状態 $s$ にのみ依存していると仮定しています．この確率分布を出力する関数 $T$ を遷移関数と呼びます．そういえば，行動 $a$ は戦略 $\\pi$ によって決定されるので，$T(a,s) = T(\\pi(s), s)$ となるわけですから，$T$ は $s$ にだけ依存します．なので，先の確率分布の式を次のように書き直すことにします．\n",
    "\n",
    "$$ Pr(s_{t+1}=s' \\mid s_t=s) = T(s) $$\n",
    "\n",
    "あとは，この確率分布を元に次の状態を決定する関数があれば，状態 $s$ から $s'$ への遷移が計算できます．この関数を $\\mathrm{choice}$ と定義することにします．\n",
    "\n",
    "$$ s' = \\mathrm{choice}(Pr) $$\n",
    "\n",
    "これで状態 $s$ から $s'$ への遷移が計算できるようになりました．次は，この遷移の良し悪しを評価するための指標を考えます．これを即時報酬 $r$ と呼び，$r$ を計算する関数 $R$ を報酬関数と呼びます．即時報酬は直前の状態と遷移先にのみ依存します．\n",
    "\n",
    "$$ r = R(s,s') $$\n",
    "\n",
    "大事なのはこの即時報酬ではなく，環境の開始から終了までの期間に得られた即時報酬の総和です．この開始から終了までの期間を１エピソードと呼びます．「開始から終了までの期間」の定義は環境によって異なりますが，例えば２次元迷路の場合なら，スタートしてからゴールに辿り着くまでが１エピソードになります．**機械学習の目的とは，この１エピソードで得られる報酬の総和を最大化することにあります．**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python で実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**numpy** を使います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$N \\times M$ の２次元迷路を実装します．迷路は行列で表し，各要素の値はそれぞれ次の内のいずれかになるとします．\n",
    "- **S** - Starting point\n",
    "- **G** - Goal point\n",
    "- **H** - Hole\n",
    "- **F** - Frozen surface\n",
    "\n",
    "`S`と`G`はそれぞれ１ずつ存在するものとします．`S`から出発して，`G`にたどり着けばゲームクリア，`H`に落ちればゲームオーバーです．`S`と`F`のマスは凍っているので，一定の確率でスリップし，望んだ方向とは別の方向に移動してしまいます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.array([\n",
    "    ['S','F','F','H'],\n",
    "    ['F','F','F','F'],\n",
    "    ['F','F','F','G'],\n",
    "])\n",
    "\n",
    "N = len(grid)\n",
    "M = len(grid[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "状態はタプルで表します．例えば，位置座標 $(0, 2)$ はそのまま `(0, 2)` と書けます．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "行動はラムダ式で表します．状態を引数にとって，遷移後の状態を返すようにします．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_up = lambda s: (max(0, s[0]-1), s[1])\n",
    "a_down = lambda s: (min(s[0]+1, N-1), s[1])\n",
    "a_left = lambda s: (s[0], max(0, s[1]-1))\n",
    "a_right = lambda s: (s[0], min(s[1]+1, M-1))\n",
    "A = [a_left, a_up, a_right, a_down]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に戦略 $\\pi$ ですが，ここではとりあえず，ランダムに行動を１つ選択するものとして定義します．利便性のために，行動(ラムダ式)本体ではなくインデックスを返すようにします．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pi(s):\n",
    "    return np.random.randint(len(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次は遷移関数です．出力である確率分布は，遷移確率行列として表します．この行列の $(i,j)$ 成分は，状態 $(i,j)$ に遷移する確率に対応しています．例えば今 $(1,1)$ にいて，行動 `a_up` を選択したとします．しかし，一定の確率（例えば $20\\%$）でスリップするため，反対方向以外の方向（左または右）にそれぞれ $10\\%$ の確率で移動してしまいます．この場合の遷移確率行列は次のようになります．\n",
    "\n",
    "$$\n",
    "Pr = \\left(\n",
    "    \\begin{array}{cccc}\n",
    "        0.0 & 0.8 & 0.0 & 0.0 \\\\\n",
    "        0.1 & 0.0 & 0.1 & 0.0 \\\\\n",
    "        0.0 & 0.0 & 0.0 & 0.0 \n",
    "    \\end{array}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "次のような場合は現在地に留まる確率が非０になります．例えば $(0,0)$ で行動 `a_up` を選択したとします．この場合 $80\\%$ で上に，$10\\%$ で左に移動しますが，上にも左にもこれ以上進めないので，どちらの場合も遷移先は $(0,0)$ になります．したがって $(0,0)$ の遷移確率は $80\\%+10\\%=90\\%$ になります．\n",
    "\n",
    "$$\n",
    "Pr = \\left(\n",
    "    \\begin{array}{cccc}\n",
    "        0.9 & 0.1 & 0.0 & 0.0 \\\\\n",
    "        0.0 & 0.0 & 0.0 & 0.0 \\\\\n",
    "        0.0 & 0.0 & 0.0 & 0.0 \n",
    "    \\end{array}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "スリップする確率は `slip_prob` で指定します．デフォルトでは，望んだ方向に進める確率が約 $\\frac{1}{3}$ になるように設定しています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def T(s, policy=pi, slip_prob=0.66):\n",
    "    pr = np.zeros((N, M))\n",
    "    idx = policy(s)\n",
    "    pr[A[idx](s)] += 1-slip_prob\n",
    "    pr[A[(idx+1)%4](s)] += slip_prob/2\n",
    "    pr[A[(idx-1)%4](s)] += slip_prob/2\n",
    "    return pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  0.8 0.  0. ]\n",
      " [0.1 0.  0.1 0. ]\n",
      " [0.  0.  0.  0. ]]\n"
     ]
    }
   ],
   "source": [
    "print(T((1,1), policy=lambda s: A.index(a_up), slip_prob=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次は $\\mathrm{choice}$ 関数です．遷移確率行列 `pr` に従って，確率的に遷移先を選択します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choice(pr):\n",
    "    r = np.random.rand()\n",
    "    p_sum = 0.0\n",
    "    for s, p in np.ndenumerate(pr):\n",
    "        p_sum += p\n",
    "        if r <= p_sum:\n",
    "            return s # state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  0.8 0.  0. ]\n",
      " [0.1 0.  0.1 0. ]\n",
      " [0.  0.  0.  0. ]]\n",
      "(0, 1)\n"
     ]
    }
   ],
   "source": [
    "pr = T((1,1), policy=lambda s: A.index(a_up), slip_prob=0.2)\n",
    "s = choice(pr)\n",
    "print(pr)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後は報酬関数です．ここでは単純に，各マスの種類に対して一定の報酬を与えることにします．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def R(s):\n",
    "    if grid[s]=='G':\n",
    "        return 1\n",
    "    elif grid[s]=='H':\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際に動かしてみます．スタート`S`から出発して，ゴール`G`に辿り着くか穴`H`に落ちるまで繰り返し遷移します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAME CLEAR!!\n",
      "It took 55 steps.\n",
      "Reward : 1.000000\n"
     ]
    }
   ],
   "source": [
    "def next_step(s):\n",
    "    s = choice(T(s))\n",
    "    r = R(s)\n",
    "    return s, r\n",
    "\n",
    "steps = 0\n",
    "reward = 0\n",
    "s = next(filter(lambda s: s[1]=='S', np.ndenumerate(grid)), (0, 0))[0] # Find starting point\n",
    "while grid[s] in ['S', 'F']:\n",
    "    s, r = next_step(s)\n",
    "    reward += r\n",
    "    steps += 1\n",
    "    \n",
    "if grid[s] == 'G':\n",
    "    print('GAME CLEAR!!')\n",
    "else:\n",
    "    print('GAME OVER...')\n",
    "\n",
    "print('It took %d steps.'%steps)\n",
    "print('Reward : %f'%reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
